{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo Parquet 'deputadosPartidos.parquet' enviado com sucesso para o Azure Blob Storage.\n",
      "Processamento dos dados de deputados concluído.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import io\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Credenciais\n",
    "account_name = 'uvabr'\n",
    "account_key = os.environ['ACCOUNT_KEY']\n",
    "container_name_send = 'gold'\n",
    "\n",
    "def extrair_dados_api(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['dados']\n",
    "    else:\n",
    "        print(f\"Erro ao acessar a API: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def transformar_dados_para_dataframe(dados):\n",
    "    df = pd.json_normalize(dados)\n",
    "    return df\n",
    "\n",
    "def enviar_parquet_azure(account_name, account_key, container_name_send, file_name, dataframe):\n",
    "    # Conecte-se ao serviço de armazenamento de blob\n",
    "    connect_str = f\"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net\"\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "\n",
    "    # Acesse o contêiner\n",
    "    container_client = blob_service_client.get_container_client(container_name_send)\n",
    "\n",
    "    # Converta DataFrame para tabela Parquet\n",
    "    table = pa.Table.from_pandas(dataframe)\n",
    "\n",
    "    # Crie um buffer de memória para escrever os dados Parquet\n",
    "    parquet_buffer = io.BytesIO()\n",
    "    pq.write_table(table, parquet_buffer)\n",
    "\n",
    "    # Envie os dados para o Azure Blob Storage\n",
    "    blob_client = container_client.get_blob_client(file_name)\n",
    "    blob_client.upload_blob(parquet_buffer.getvalue(), overwrite=True)\n",
    "\n",
    "    print(f\"Arquivo Parquet '{file_name}' enviado com sucesso para o Azure Blob Storage.\")\n",
    "\n",
    "def processar_dados_deputados(account_name, account_key, container_name_send):\n",
    "    url = \"https://dadosabertos.camara.leg.br/api/v2/deputados\"\n",
    "    dados = extrair_dados_api(url)\n",
    "    if dados:\n",
    "        df = transformar_dados_para_dataframe(dados)\n",
    "        enviar_parquet_azure(account_name, account_key, container_name_send, 'deputadosPartidos.parquet', df)\n",
    "        print(\"Processamento dos dados de deputados concluído.\")\n",
    "\n",
    "# Processar dados dos deputados\n",
    "processar_dados_deputados(account_name, account_key, container_name_send)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESSE SCRIPT TEM COMO OBJETIVO DE EXTRAIR TODOS OS DADOS DOS DEPUTADOS E DOS PARTIDOS CRIANDO ARQUIVO deputadosCompleto e partidosCompleto na bronze\n",
    "# \n",
    "# \n",
    "import pandas as pd\n",
    "import requests\n",
    "import pyarrow as pa\n",
    "import io\n",
    "import pyarrow.parquet as pq\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "\n",
    "\n",
    "# Substitua pelos valores reais obtidos no Passo 1\n",
    "account_name = 'uvabr'\n",
    "account_key = os.environ['ACCOUNT_KEY']\n",
    "container_name = 'bronze'\n",
    "\n",
    "# Conecte-se ao serviço de armazenamento de blob\n",
    "connect_str = f\"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net\"\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "\n",
    "# Acesse o contêiner\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "\n",
    "# Função para extrair dados da API\n",
    "def extrair_dados_api(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['dados']\n",
    "    else:\n",
    "        print(f\"Erro ao acessar a API: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Função para transformar dados em dataframe\n",
    "def transformar_dados_para_dataframe(dados):\n",
    "    df = pd.json_normalize(dados)\n",
    "    return df\n",
    "\n",
    "# Função para extrair dados das URIs em paralelo\n",
    "def extrair_dados_uris(uris):\n",
    "    dados_completos = []\n",
    "    contagem = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_url = {executor.submit(extrair_dados_api, uri): uri for uri in uris}\n",
    "        for future in as_completed(future_to_url):\n",
    "            uri = future_to_url[future]\n",
    "            try:\n",
    "                dados_api = future.result()\n",
    "                if dados_api:\n",
    "                    dados_completos.append(dados_api)\n",
    "                contagem += 1\n",
    "                print(f\"Processando {contagem}: {uri}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar {uri}: {e}\")\n",
    "\n",
    "    if dados_completos:\n",
    "        df_completo = pd.json_normalize(dados_completos)\n",
    "        return df_completo\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "def enviar_parquet_azure(account_name, account_key, container_name_send, file_name, dataframe):\n",
    "    # Conecte-se ao serviço de armazenamento de blob\n",
    "    connect_str = f\"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net\"\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "\n",
    "    # Acesse o contêiner\n",
    "    container_client = blob_service_client.get_container_client(container_name_send)\n",
    "\n",
    "    # Converta DataFrame para tabela Parquet\n",
    "    table = pa.Table.from_pandas(dataframe)\n",
    "\n",
    "    # Crie um buffer de memória para escrever os dados Parquet\n",
    "    parquet_buffer = io.BytesIO()\n",
    "    pq.write_table(table, parquet_buffer)\n",
    "\n",
    "    # Envie os dados para o Azure Blob Storage\n",
    "    blob_client = container_client.get_blob_client(file_name)\n",
    "    blob_client.upload_blob(parquet_buffer.getvalue(), overwrite=True)\n",
    "\n",
    "    print(f\"Arquivo Parquet '{file_name}' enviado com sucesso para o Azure Blob Storage.\")\n",
    "    \n",
    "# URL para extrair dados iniciais dos deputados\n",
    "url = \"https://dadosabertos.camara.leg.br/arquivos/deputados/json/deputados.json\"\n",
    "dados = extrair_dados_api(url)\n",
    "df = transformar_dados_para_dataframe(dados)\n",
    "\n",
    "# Extrair dados das URIs dos deputados e criar o dataframe df_deputados\n",
    "df_deputados = extrair_dados_uris(df['uri'])\n",
    "enviar_parquet_azure(account_name, account_key, 'bronze', f'deputadosCompleto.parquet', df_deputados)\n",
    "\n",
    "# Remover duplicatas do campo 'ultimoStatus.uriPartido' em df_deputados\n",
    "uris_partidos_unicos = df_deputados['ultimoStatus.uriPartido'].drop_duplicates()\n",
    "\n",
    "# Extrair dados das URIs dos partidos e criar um novo dataframe\n",
    "df_partidos = extrair_dados_uris(uris_partidos_unicos)\n",
    "enviar_parquet_azure(account_name, account_key, 'bronze', f'partidosCompleto.parquet', df_partidos)\n",
    "\n",
    "enviar_parquet_azure(account_name, account_key, 'gold', f'deputadosCompleto.parquet', df_deputados)\n",
    "enviar_parquet_azure(account_name, account_key, 'gold', f'partidosCompleto.parquet', df_partidos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
